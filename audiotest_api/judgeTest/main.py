#!/usr/bin/env python3
"""
íŒë‹¨ ì„œë²„ (Judge Server)
ì²­í¬ë¥¼ ë°›ì•„ì„œ VAD ì²˜ë¦¬í•˜ê³  status ë°˜í™˜
"""
import librosa

from uuid import uuid4
import asyncio
import dataclasses
import shutil
from openai import OpenAI
import time
from silero_vad import load_silero_vad, get_speech_timestamps
import soundfile as sf
import numpy as np
import os
from dotenv import load_dotenv
from fastapi import FastAPI, UploadFile, Form, Response
from fastapi.responses import JSONResponse
from pathlib import Path
from typing import Dict
from fastapi.middleware.cors import CORSMiddleware
import json
from collections import deque

active_sessions = deque(maxlen=100)

load_dotenv()

# ========== ì„¤ì • í´ë˜ìŠ¤ ==========
@dataclasses.dataclass
class AudioConfig:
    """ì˜¤ë””ì˜¤ ì„¤ì •"""
    SAMPLERATE: int = 16000
    SILENCE_THRESHOLD: int = 3
    EXIT_THRESHOLD: int = 10
    GAIN: float = 3.0
    VAD_THRESHOLD: float = 0.2
    WHISPER_MODEL: str = "whisper-1"
    WHISPER_LANGUAGE: str = "ko"


@dataclasses.dataclass
class ServerConfig:
    """ì„œë²„ ì„¤ì •"""
    HOST: str = "127.0.0.1"
    # HOST: str = "192.168.0.37"
    PORT: int = 8000


@dataclasses.dataclass
class CORSConfig:
    """CORS ì„¤ì •"""
    ALLOW_ORIGINS: list = dataclasses.field(default_factory=lambda: ["*"])
    ALLOW_CREDENTIALS: bool = True
    ALLOW_METHODS: list = dataclasses.field(default_factory=lambda: ["*"])
    ALLOW_HEADERS: list = dataclasses.field(default_factory=lambda: ["*"])


@dataclasses.dataclass
class PathConfig:
    """ê²½ë¡œ ì„¤ì •"""
    SESSIONS_DIR: str = "sessions_b"
    INBOX_DIR: str = "inbox"
    TEMP_FILE_PREFIX: str = "temp_audio_"


@dataclasses.dataclass
class VADConfig:
    """VAD ëª¨ë¸ ì„¤ì •"""
    MONITORING: bool = False


# ========== Config ë¡œë” ==========
def load_config(config_path: str = "config.json"):
    """JSON ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    config_file = Path(config_path)
    
    if config_file.exists():
        print(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ: {config_path}")
        with open(config_file, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
    else:
        print(f"âš ï¸  ì„¤ì • íŒŒì¼ ì—†ìŒ. ê¸°ë³¸ê°’ ì‚¬ìš©: {config_path}")
        config_data = {}
    
    # AudioConfig
    audio_conf = config_data.get("audio", {})
    audio_config = AudioConfig(
        SAMPLERATE=audio_conf.get("samplerate", 16000),
        SILENCE_THRESHOLD=audio_conf.get("silence_threshold", 3),
        EXIT_THRESHOLD=audio_conf.get("exit_threshold", 10),
        GAIN=audio_conf.get("gain", 3.0),
        VAD_THRESHOLD=audio_conf.get("vad_threshold", 0.2),
        WHISPER_MODEL=audio_conf.get("whisper_model", "whisper-1"),
        WHISPER_LANGUAGE=audio_conf.get("whisper_language", "ko")
    )
    
    # ServerConfig
    server_conf = config_data.get("server", {})
    server_config = ServerConfig(
        HOST=server_conf.get("host", "127.0.0.1"),
        PORT=server_conf.get("port", 8000)
    )
    
    # CORSConfig
    cors_conf = config_data.get("cors", {})
    cors_config = CORSConfig(
        ALLOW_ORIGINS=cors_conf.get("allow_origins", ["*"]),
        ALLOW_CREDENTIALS=cors_conf.get("allow_credentials", True),
        ALLOW_METHODS=cors_conf.get("allow_methods", ["*"]),
        ALLOW_HEADERS=cors_conf.get("allow_headers", ["*"])
    )
    
    # PathConfig
    path_conf = config_data.get("paths", {})
    path_config = PathConfig(
        SESSIONS_DIR=path_conf.get("sessions_dir", "sessions_b"),
        INBOX_DIR=path_conf.get("inbox_dir", "inbox"),
        TEMP_FILE_PREFIX=path_conf.get("temp_file_prefix", "temp_audio_")
    )
    
    # VADConfig
    vad_conf = config_data.get("vad", {})
    vad_config = VADConfig(
        MONITORING=vad_conf.get("monitoring", False)
    )
    
    return audio_config, server_config, cors_config, path_config, vad_config


# ========== ì„¤ì • ë¡œë“œ ==========
AUDIO_CONFIG, SERVER_CONFIG, CORS_CONFIG, PATH_CONFIG, VAD_CONFIG = load_config()

# FastAPI ì•±
app = FastAPI()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=CORS_CONFIG.ALLOW_ORIGINS,
    allow_credentials=CORS_CONFIG.ALLOW_CREDENTIALS,
    allow_methods=CORS_CONFIG.ALLOW_METHODS,
    allow_headers=CORS_CONFIG.ALLOW_HEADERS,
)

# ê²½ë¡œ ì„¤ì •
BASE = Path(__file__).parent
SESS_BASE = BASE / PATH_CONFIG.SESSIONS_DIR
SESS_BASE.mkdir(exist_ok=True)
INBOX = BASE / PATH_CONFIG.INBOX_DIR
INBOX.mkdir(exist_ok=True)

# OpenAI API í‚¤ ì„¤ì •
api_key = os.getenv("OPENAI_API_KEY", "your-api-key")
client = OpenAI(api_key=api_key)


# ========== VAD ëª¨ë¸ ==========
class VADModel:
    """VAD ëª¨ë¸ ë˜í¼ í´ë˜ìŠ¤"""
    def __init__(self, audio_config: AudioConfig, vad_config: VADConfig) -> None:
        self.model = load_silero_vad()
        self.SAMPLERATE = audio_config.SAMPLERATE
        self.VAD_THRESHOLD = audio_config.VAD_THRESHOLD
        self.monitoring = vad_config.MONITORING

    def get_speech_timestamps(self, audio_data) -> list:
        """ì˜¤ë””ì˜¤ ë°ì´í„°ì—ì„œ ìŒì„± êµ¬ê°„ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë°˜í™˜"""
        if self.monitoring:
            print(f"[VAD] audio_data type: {type(audio_data)}")
            print(f"[VAD] audio_data dtype: {audio_data.dtype}")
            print(f"[VAD] audio_data shape: {audio_data.shape}")
            print(f"[VAD] audio_data range: [{audio_data.min():.4f}, {audio_data.max():.4f}]")
  
        return get_speech_timestamps(
            audio_data,
            self.model,
            threshold=self.VAD_THRESHOLD,
            sampling_rate=self.SAMPLERATE,
        )


# ========== ìŒì„± í™œë™ ê°ì§€ ==========
class _AudioActivityDetection:
    """ìŒì„± í™œë™ ê°ì§€ í´ë˜ìŠ¤"""
    def __init__(self, audio_config: AudioConfig):
        self.is_recording = False
        self.speech_buffer = []
        self.stop_count = 0
        self.silence_threshold = audio_config.SILENCE_THRESHOLD
        self.exit_threshold = audio_config.EXIT_THRESHOLD

    def resetStream(self):
        """ìŠ¤íŠ¸ë¦¼ ìƒíƒœ ì´ˆê¸°í™”"""
        self.is_recording = False
        self.speech_buffer = []
        self.stop_count = 0
        return {"audio": None, "status": "Reset"}

    def __call__(self, speech_detected: list, audio_buffer: np.array) -> dict:
        """ìŒì„± ë°ì´í„°ì—ì„œ í™”ì í™œë™ì„ ê°ì§€"""
        has_speech = len(speech_detected) > 0
        user_status = "Silent"
        user_audio = None
        
        if has_speech:
            if not self.is_recording:
                self.is_recording = True
                self.stop_count = 0
                self.speech_buffer = []
                user_status = "Speech"
                print("ğŸ¤ ìŒì„± ì‹œì‘")
            else:
                user_status = "Speech"
            
            self.speech_buffer.append(audio_buffer)
            
            if self.stop_count > 0:
                print(f"ìŒì„± ì¬ê°ì§€ â†’ ë¬´ìŒ ì¹´ìš´íŠ¸ ë¦¬ì…‹ ({self.stop_count} â†’ 0)")
                self.stop_count = 0
            
        else:  # ë¬´ìŒ
            if self.is_recording:
                zero_data = np.zeros_like(audio_buffer)
                self.speech_buffer.append(zero_data)
                self.stop_count += 1
                user_status = "Speech"
                
                print(f"ì—°ì† ë¬´ìŒ: {self.stop_count}/{self.silence_threshold}")
                
                if self.stop_count >= self.silence_threshold:
                    speech_data = np.concatenate(self.speech_buffer, axis=0)
                    self.is_recording = False
                    self.stop_count = 0
                    self.speech_buffer = []
                    user_audio = speech_data
                    user_status = "Finished"
                    print("âœ… ìŒì„± ì¢…ë£Œ")
                    
            else:
                self.stop_count += 1
                if self.stop_count >= self.exit_threshold:
                    print(f"âŒ ì—°ì† {self.exit_threshold}ë²ˆ ë¬´ìŒìœ¼ë¡œ ì‹œìŠ¤í…œ ì¢…ë£Œ")
                    user_audio = None
                    user_status = "Error"
                else:
                    user_status = "Silent"

        return {"audio": user_audio, "status": user_status}


# ì„¸ì…˜ ìƒíƒœ ë° VAD ëª¨ë¸ ì´ˆê¸°í™”
session_states: Dict[str, _AudioActivityDetection] = {}
_vad_model = VADModel(AUDIO_CONFIG, VAD_CONFIG)


# ========== í•µì‹¬ í•¨ìˆ˜: ì˜¤ë””ì˜¤ ì²­í¬ ì²˜ë¦¬ ==========
async def process_audio_chunk(session_id: str, audio_data, reset: bool = False) -> dict:
    """ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì²­ì·¨ ë° í…ìŠ¤íŠ¸ ë³€í™˜"""
    vad_model = _vad_model
    audio_data = librosa.resample(audio_data, orig_sr=48000, target_sr=16000)

    if session_id not in session_states:
        session_states[session_id] = _AudioActivityDetection(AUDIO_CONFIG)

    event_checker = session_states[session_id]    
    
    result_status = None
    transcript_text = None

    if reset:
        result = event_checker.resetStream()
        return {"status": result["status"], "text": None}

    if audio_data is not None:
        speech_timestamps = vad_model.get_speech_timestamps(audio_data)
        result = event_checker(speech_timestamps, audio_data)
        
        result_status = result["status"]
                
        if result["audio"] is not None:
            # ì„ì‹œ íŒŒì¼ ì´ë¦„ ìƒì„±
            temp_file_name = f"{PATH_CONFIG.TEMP_FILE_PREFIX}{session_id}_{time.time()}.wav"
            
            # íŒŒì¼ ì“°ê¸°
            await asyncio.to_thread(
                sf.write, 
                temp_file_name, 
                result["audio"], 
                AUDIO_CONFIG.SAMPLERATE
            )
            
            # STT í˜¸ì¶œ
            def transcribe_sync():
                with open(temp_file_name, "rb") as audio_file:
                    return client.audio.transcriptions.create(
                        model=AUDIO_CONFIG.WHISPER_MODEL,
                        file=audio_file,
                        language=AUDIO_CONFIG.WHISPER_LANGUAGE
                    )

            response = await asyncio.to_thread(transcribe_sync)
            transcript_text = response.text
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                
            os.makedirs("audio_data", exist_ok=True)
            save_path = f"audio_data/{session_id}_{time.time()}.wav"
            await asyncio.to_thread(shutil.copy, temp_file_name, save_path)            
            await asyncio.to_thread(os.remove, temp_file_name)
                        
            print(f"ğŸ“ ì¸ì‹ëœ í…ìŠ¤íŠ¸: {transcript_text}")

        elif result["status"] in ["Error", "Speech", "Silent", "Reset"]:
            transcript_text = None

    else:
        result_status = "silent"
        transcript_text = None
        
    if result_status in ["Finished", "Error"]:
        print(f"ì„¸ì…˜ {session_id} ìƒíƒœ ì •ë¦¬.")
        if session_id in session_states:
            del session_states[session_id]
                    
    return {"status": result_status, "text": transcript_text}


# ========== FastAPI ë¼ìš°íŠ¸ ==========
@app.post("/start")
def start():
    """ìƒˆ ì„¸ì…˜ ì‹œì‘ (API í˜¸í™˜ì„± ìœ ì§€)"""
    sid = str(uuid4())
    active_sessions.append(sid)
    return {"sessionId": sid}


@app.post("/ingest-chunk")
async def ingest_chunk(
    sessionId: str = Form(...),
    chunk: UploadFile = Form(...),
    mode: str = Form("chunk")  # "chunk" ë˜ëŠ” "file"
):
    """ì²­í¬/íŒŒì¼ ìˆ˜ì‹  â†’ VAD ì²˜ë¦¬ ë˜ëŠ” ì§ì ‘ ì „ì‚¬ â†’ ì‘ë‹µ ë°˜í™˜"""
    #í•¨ìˆ˜ ì‹œì‘ì „ì— ë¬´ì¡°ê»€ session ID ì¤‘ë³µê²€ì‚¬ë¥¼ ì¤‘ë³µì´ë©´ ì—ëŸ¬ë¡œ ë°˜í™˜í•¨
    if sessionId not in active_sessions:
        return JSONResponse({
            "status": "Error",
            "text": None,
            "detail": "Invalid sessionId. Call /start first."
        }, status_code=400)    
    
    try:
        chunk_data = await chunk.read()
        print(f"ğŸ“¥ [íŒë‹¨] ì„¸ì…˜: {sessionId[:8]}... | ëª¨ë“œ: {mode} | í¬ê¸°: {len(chunk_data)} bytes")
        
        # ========== íŒŒì¼ ëª¨ë“œ: ë°”ë¡œ Whisper ì „ì‚¬ ==========
        if mode == "file":
            temp_path = f"{PATH_CONFIG.TEMP_FILE_PREFIX}{sessionId}_{time.time()}.wav"
            
            with open(temp_path, "wb") as f:
                f.write(chunk_data)
            
            def transcribe_sync():
                with open(temp_path, "rb") as audio_file:
                    return client.audio.transcriptions.create(
                        model=AUDIO_CONFIG.WHISPER_MODEL,
                        file=audio_file,
                        language=AUDIO_CONFIG.WHISPER_LANGUAGE
                    )
            
            response = await asyncio.to_thread(transcribe_sync)
            await asyncio.to_thread(os.remove, temp_path)
            
            print(f"ğŸ“ [íŒŒì¼ëª¨ë“œ] ì¸ì‹ëœ í…ìŠ¤íŠ¸: {response.text}")
            
            return JSONResponse({
                "status": "Finished",
                "text": response.text
            }, status_code=200)
        
        # ========== ì²­í¬ ëª¨ë“œ: VAD ì²˜ë¦¬ ==========
        else:
            audio_array = np.frombuffer(chunk_data, dtype=np.int16)
            audio_data = audio_array.astype(np.float32) / 32768.0
            
            print(f"ğŸ”„ [íŒë‹¨] ìƒ˜í”Œ ìˆ˜: {len(audio_data)} | ë²”ìœ„: [{audio_data.min():.3f}, {audio_data.max():.3f}]")
            
            audio_data = audio_data * AUDIO_CONFIG.GAIN
            result = await process_audio_chunk(sessionId, audio_data)
            
            print(f"ğŸ¯ [íŒë‹¨] VAD ê²°ê³¼: {result['status']}")
            
            if result["status"] == "Error":
                return JSONResponse({
                    "status": "Error",
                    "text": None
                }, status_code=500)
            elif result["status"] == "Speech":
                return JSONResponse({
                    "status": "Speech",
                    "text": None
                }, status_code=200)
            
            elif result["status"] == "Finished":
                return JSONResponse({
                    "status": "Finished",
                    "text": result["text"]
                }, status_code=200)
            
            else: #Silent
                return JSONResponse({
                    "status": "Silent",
                    "text": None
                }, status_code=200)

    except Exception as e:
        print(f"âŒ ì—ëŸ¬: {str(e)}")
        import traceback
        traceback.print_exc()
        return JSONResponse({
            "status": "Error",
            "text": None,
            "detail": str(e)
        }, status_code=500)

# ========== CLI ëª¨ë“œ ==========
if __name__ == '__main__':
    import uvicorn
    print(f"ğŸš€ íŒë‹¨ ì„œë²„ ì‹œì‘: {SERVER_CONFIG.HOST}:{SERVER_CONFIG.PORT}")
    uvicorn.run(app, host=SERVER_CONFIG.HOST, port=SERVER_CONFIG.PORT)