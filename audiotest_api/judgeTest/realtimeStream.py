#!/usr/bin/env python3
"""
íŒë‹¨ ì„œë²„ (Judge Server)
ì²­í¬ë¥¼ ë°›ì•„ì„œ VAD ì²˜ë¦¬í•˜ê³  status ë°˜í™˜
"""
from fastapi.responses import FileResponse  # â† ì´ê±° ì¶”ê°€
from uuid import uuid4

import dataclasses
from openai import OpenAI
import time
from silero_vad import load_silero_vad, get_speech_timestamps
import soundfile as sf
import numpy as np
import librosa
import os
from dotenv import load_dotenv
from fastapi import FastAPI, UploadFile, Form, Response
from fastapi.responses import JSONResponse
from pathlib import Path
from typing import Dict
import shutil
import io
from fastapi.middleware.cors import CORSMiddleware


load_dotenv()

# FastAPI ì•±
app = FastAPI()

# CORS ì„¤ì • (í•„ìˆ˜!)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

BASE = Path(__file__).parent
SESS_BASE = BASE / "sessions_b"
SESS_BASE.mkdir(exist_ok=True)
INBOX = BASE / "inbox"
INBOX.mkdir(exist_ok=True)

# OpenAI API í‚¤ ì„¤ì •
api_key = os.getenv("OPENAI_API_KEY", "your-api-key")
client = OpenAI(api_key=api_key)

# ========== ì„¤ì • ==========
@dataclasses.dataclass
class AudioConfig:
    """ì˜¤ë””ì˜¤ ì„¤ì • ìƒìˆ˜"""
    SAMPLERATE = 16000
    SILENCE_THRESHOLD = 3
    EXIT_THRESHOLD = 10


# ========== VAD ëª¨ë¸ ==========
class VADModel:
    """
    ìŒì„±ì„ ê°ì§€í•˜ëŠ” VAD ëª¨ë¸ ë˜í¼ í´ë˜ìŠ¤ (private)
    ìƒì„±í•˜ë©´ ë™ì‹œì— VAD ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Attributes:
        model: ë¡œë“œëœ VAD ëª¨ë¸
    """
    def __init__(self, monitoring=False) -> None:
        self.model = load_silero_vad()
        self.SAMPLERATE = AudioConfig.SAMPLERATE
        self.monitoring = monitoring

    def get_speech_timestamps(self, audio_data) -> list:
        """
        ì˜¤ë””ì˜¤ ë°ì´í„°ì—ì„œ ìŒì„± êµ¬ê°„ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            audio_data (np.array): ì˜¤ë””ì˜¤ ì‹ í˜¸ ë°°ì—´

        Returns:
            list: ê°ì§€ëœ ìŒì„± êµ¬ê°„ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸
        """
        if self.monitoring:
            print(f"[VAD] audio_data type: {type(audio_data)}")
            print(f"[VAD] audio_data dtype: {audio_data.dtype}")
            print(f"[VAD] audio_data shape: {audio_data.shape}")
            print(f"[VAD] audio_data range: [{audio_data.min():.4f}, {audio_data.max():.4f}]")
  
        return get_speech_timestamps(
            audio_data,
            self.model,
            threshold=0.2,
            sampling_rate=self.SAMPLERATE,
        )


# ========== ìŒì„± í™œë™ ê°ì§€ ==========
class _AudioActivityDetection:
    """
    ìŒì„± ë°ì´í„°ë¥¼ ì½ì–´ ì™€ì„œ í™”ìê°€ ëŒ€í™”ë¥¼ í•˜ê³  ìˆëŠ”ì§€ ê°ì‹œ
    Status
    - 1. Silent: ë¬´ìŒ ìƒíƒœ
    - 2. Speech: ìŒì„± ê°ì§€ ìƒíƒœ
    - 3. Finished: ìŒì„± ë…¹ìŒ ì¢…ë£Œ ìƒíƒœ
    - 4. Error: ì—°ì† ë¬´ìŒìœ¼ë¡œ ì¸í•œ ì‹œìŠ¤í…œ ì¢…ë£Œ ìƒíƒœ
    - 5. Reset: ìŠ¤íŠ¸ë¦¼ ìƒíƒœ ì´ˆê¸°í™” ìƒíƒœ
    
    Attributes:
        is_recording:  í˜„ì¬ ë…¹ìŒì¤‘ì¸ ì—¬ë¶€ë¡œ ìµœì´ˆë¡œ ìŒì„±ì´ ê°ì§€ë˜ë©´ Trueë¡œ ë³€ê²½ë˜ê³ ,
                       ì—°ì†ìœ¼ë¡œ ë¬´ìŒì´ silence_thresholdë²ˆ ê°ì§€ë˜ë©´ Falseë¡œ ë³€ê²½ë©ë‹ˆë‹¤.
        speech_buffer: ë…¹ìŒëœ ìŒì„± ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ë²„í¼  
        stop_count: ì—°ì† ë¬´ìŒ ì¹´ìš´íŠ¸
        silence_threshold: ì—°ì† ë¬´ìŒìœ¼ë¡œ ê°„ì£¼í•˜ëŠ” ì„ê³„ê°’
        exit_threshold: ì—°ì† ë¬´ìŒìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ì‹œìŠ¤í…œ ì¢…ë£Œí•˜ëŠ” ì„ê³„ê°’
    
    Methods:
        resetStream(): ìŠ¤íŠ¸ë¦¼ ìƒíƒœ ì´ˆê¸°í™”
        __call__(speech_detected, audio_buffer): ìŒì„± ë°ì´í„°ì—ì„œ í™”ì í™œë™ì„ ê°ì§€í•˜ê³  ë…¹ìŒ ì‹œì‘/ì¢…ë£Œë¥¼ ì œì–´

    """
    def __init__(self, 
                 silence_threshold: int = AudioConfig.SILENCE_THRESHOLD,
                 exit_threshold: int = AudioConfig.EXIT_THRESHOLD):
        self.is_recording = False
        self.speech_buffer = []
        self.stop_count = 0
        self.silence_threshold = silence_threshold
        self.exit_threshold = exit_threshold

    def resetStream(self):
        """ìŠ¤íŠ¸ë¦¼ ìƒíƒœ ì´ˆê¸°í™”"""
        self.is_recording = False
        self.speech_buffer = []
        self.stop_count = 0
        return {"audio": None, "status": "Reset"}

    def __call__(self, 
                 speech_detected: list,
                 audio_buffer: np.array) -> dict:
        """
        ìŒì„± ë°ì´í„°ì—ì„œ í™”ì í™œë™ì„ ê°ì§€í•˜ê³  ë…¹ìŒ ì‹œì‘/ì¢…ë£Œë¥¼ ì œì–´í•©ë‹ˆë‹¤.

        Args:
            speech_detected (list): ê°ì§€ëœ ìŒì„± êµ¬ê°„ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸
            audio_buffer (np.array): í˜„ì¬ ì˜¤ë””ì˜¤ ë²„í¼ ë°ì´í„°
        Returns:
            dict: {
                "audio": ì™„ì„±ëœ ìŒì„± ë°ì´í„° ë°°ì—´ ë˜ëŠ” None,
                "status": "Silent" | "Speech" | "Finished" | "Error",
                "decision": "start" | "end" | None
            }
        """
        has_speech = len(speech_detected) > 0
        user_status = "Silent"
        user_audio = None
        
        if has_speech:
            if not self.is_recording:
                self.is_recording = True
                self.stop_count = 0
                self.speech_buffer = []
                user_status = "Speech"
                decision = "start"  # ìŒì„± ì‹œì‘!
                print("ğŸ¤ ìŒì„± ì‹œì‘ (decision: start)")
            else:
                user_status = "Speech"
            
            self.speech_buffer.append(audio_buffer)
            
            if self.stop_count > 0:
                print(f"ìŒì„± ì¬ê°ì§€ â†’ ë¬´ìŒ ì¹´ìš´íŠ¸ ë¦¬ì…‹ ({self.stop_count} â†’ 0)")
                self.stop_count = 0
            
        else:  # ë¬´ìŒ
            if self.is_recording:
                zero_data = np.zeros_like(audio_buffer)
                self.speech_buffer.append(zero_data)
                self.stop_count += 1
                user_status = "Speech"
                
                print(f"ì—°ì† ë¬´ìŒ: {self.stop_count}/{self.silence_threshold}")
                
                if self.stop_count >= self.silence_threshold:
                    speech_data = np.concatenate(self.speech_buffer, axis=0)
                    self.is_recording = False
                    self.stop_count = 0
                    self.speech_buffer = []
                    user_audio = speech_data
                    user_status = "Finished"
                    decision = "end"  # ìŒì„± ì¢…ë£Œ!
                    print("âœ… ìŒì„± ì¢…ë£Œ (decision: end)")
                    
            else:
                self.stop_count += 1
                if self.stop_count >= self.exit_threshold:
                    print(f"âŒ ì—°ì† {self.exit_threshold}ë²ˆ ë¬´ìŒìœ¼ë¡œ ì‹œìŠ¤í…œ ì¢…ë£Œ")
                    user_audio = None
                    user_status = "Error"
                else:
                    user_status = "Silent"

        return {"audio": user_audio, "status": user_status}


# ì„¸ì…˜ë³„ ì´ë²¤íŠ¸ ì²´ì»¤ (ê° ì„¸ì…˜ë§ˆë‹¤ ë…ë¦½ì ì¸ ìƒíƒœ ìœ ì§€)
session_event_checkers = _AudioActivityDetection()
_vad_model = VADModel(monitoring=False)


# ========== í•µì‹¬ í•¨ìˆ˜: ì˜¤ë””ì˜¤ ì²­í¬ ì²˜ë¦¬ ==========
def process_audio_chunk(session_id: str, audio_data, reset: bool = False) -> dict:
    """
    ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì²­ì·¨ ë° í…ìŠ¤íŠ¸ ë³€í™˜ ë‚´ë¶€ í•¨ìˆ˜
    
    Status
    - 1. Silent: ë¬´ìŒ ìƒíƒœ
    - 2. Speech: ìŒì„± ê°ì§€ ìƒíƒœ
    - 3. Finished: ìŒì„± ë…¹ìŒ ì¢…ë£Œ ìƒíƒœ
    - 4. Error: ì—°ì† ë¬´ìŒìœ¼ë¡œ ì¸í•œ ì‹œìŠ¤í…œ ì¢…ë£Œ ìƒíƒœ
    - 5. Reset: ìŠ¤íŠ¸ë¦¼ ìƒíƒœ ì´ˆê¸°í™” ìƒíƒœ
    
    Args:
        session_id (str): ì„¸ì…˜ ID
        audio_data (np.array): ì˜¤ë””ì˜¤ ì‹ í˜¸ ë°°ì—´
        reset (bool): ìŠ¤íŠ¸ë¦¼ ìƒíƒœ ì´ˆê¸°í™” ì—¬ë¶€
    Returns:
        dict: {
            "status": "Silent" | "Speech" | "Finished" | "Error" | "Reset",
            "text": ë³€í™˜ëœ í…ìŠ¤íŠ¸ ë˜ëŠ” None
        }
    """
    
    event_checker = session_event_checkers
    vad_model = _vad_model
    
    result_status = None
    transcript_text = None

    if reset:
        result = event_checker.resetStream()
        return {"status": result["status"], "text": None}

    if audio_data is not None:
        speech_timestamps = vad_model.get_speech_timestamps(audio_data)
        result = event_checker(speech_timestamps, audio_data)
        
        result_status = result["status"]
                
        if result["audio"] is not None:
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            sf.write("temp_audio.wav", result["audio"], samplerate=AudioConfig.SAMPLERATE)

            # OpenAI Whisper APIë¡œ ì „ì‚¬
            with open("temp_audio.wav", "rb") as audio_file:
                response = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="ko"
                )

            transcript_text = response.text
            print(f"ğŸ“ ì¸ì‹ëœ í…ìŠ¤íŠ¸: {transcript_text}")

        elif result["status"] == "Error":
            transcript_text = None

        elif result["status"] in ["Speech", "Silent"]:
            transcript_text = None

        elif result["status"] == "Reset":
            transcript_text = None
    else:
        result_status = "silent"
        transcript_text = None
                    
    return {"status": result_status, "text": transcript_text}   
                
# ========== CLI í…ŒìŠ¤íŠ¸ ëª¨ë“œ ==========
if __name__ == '__main__':
    import sys
    
    print("=" * 60)
    print("ğŸ¤ ì˜¤ë””ì˜¤ ì²­í¬ í…ŒìŠ¤íŠ¸ (0.5ì´ˆ ë‹¨ìœ„)")
    print("=" * 60)
    
    # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì…ë ¥
    audio_file = "D:/007.Portfolio/audioTest/judgeTest/GIK_Listening_1_Track 085.mp3"
    
    if not os.path.exists(audio_file):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_file}")
        sys.exit(1)
    
    # ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
    print(f"\nğŸ“‚ íŒŒì¼ ë¡œë”© ì¤‘: {audio_file}")
    audio_data, sr = librosa.load(
        audio_file, 
        sr=16000  # ìƒ˜í”Œë ˆì´íŠ¸ ì§€ì • (ë¦¬ìƒ˜í”Œë§ í¬í•¨)
        # mono=True ì˜µì…˜ì€ ê¸°ë³¸ê°’ì´ë¼ ìƒëµí•´ë„ ë¨
    )
    # 0.5ì´ˆ ì²­í¬ í¬ê¸° ê³„ì‚°
    chunk_size = int(AudioConfig.SAMPLERATE * 0.5)  # 8000 ìƒ˜í”Œ
    total_chunks = int(np.ceil(len(audio_data) / chunk_size))
    
    print(f"\nğŸ“Š ì´ {total_chunks}ê°œ ì²­í¬ (ê° 0.5ì´ˆ)")
    print("=" * 60)
    
    # ì„¸ì…˜ ID
    session_id = "test-session-" + str(uuid4())[:8]
    
    # ì²­í¬ë³„ ì²˜ë¦¬
    for i in range(total_chunks):
        start_idx = i * chunk_size
        end_idx = min(start_idx + chunk_size, len(audio_data))
        
        # ì²­í¬ ì¶”ì¶œ
        chunk = audio_data[start_idx:end_idx]
        
        # ë§ˆì§€ë§‰ ì²­í¬ íŒ¨ë”© (8000 ìƒ˜í”Œ ë§ì¶”ê¸°)
        if len(chunk) < chunk_size:
            chunk = np.pad(chunk, (0, chunk_size - len(chunk)))
        
        # VAD + STT ì²˜ë¦¬
        result = process_audio_chunk(session_id, chunk)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\n[ì²­í¬ #{i+1}/{total_chunks}] ({start_idx/AudioConfig.SAMPLERATE:.1f}s ~ {end_idx/AudioConfig.SAMPLERATE:.1f}s)")
        print(f"  ğŸ“Š Status: {result['status']}")
        
        if result['text']:
            print(f"  ğŸ“ Text: {result['text']}")
            print("\n" + "=" * 60)
            print("âœ… ìŒì„± ì¸ì‹ ì™„ë£Œ!")
            break
        
        # 0.5ì´ˆ ëŒ€ê¸° (ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜)
        time.sleep(0.5)
    
    print("\n" + "=" * 60)
    print("ğŸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")