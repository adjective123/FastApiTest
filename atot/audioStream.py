#!/usr/bin/env python3
"""ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ"""
import queue
import sys
from openai import OpenAI
import time
from silero_vad import load_silero_vad, get_speech_timestamps
import soundfile as sf
from fastapi import UploadFile, File
import numpy as np
import sounddevice as sd
import os
from dotenv import load_dotenv

load_dotenv()  # ì´ ì¤„ì´ ë°˜ë“œì‹œ ìˆì–´ì•¼ í•¨


# OpenAI API í‚¤ ì„¤ì •
api_key = os.getenv("OPENAI_API_KEY", "your-api-key")
client = OpenAI(api_key=api_key)

# ì „ì—­ VAD ëª¨ë¸ (ì‹±ê¸€í†¤ íŒ¨í„´)
_vad_model = None

class AudioConfig:
    """ì˜¤ë””ì˜¤ ì„¤ì • ìƒìˆ˜"""
    DEVICE = None
    SAMPLERATE = 16000
    CHANNELS = 1
    CHUNKSIZE = 64
    BATCH_SIZE = 100
    SILENCE_THRESHOLD = 3

class _VADModel:
    """ë‚´ë¶€ VAD ëª¨ë¸ (private)"""
    def __init__(self):
        self.model = load_silero_vad()
    
    def get_speech_timestamps(self, audio_data):
        return get_speech_timestamps(
            audio_data,
            self.model,
            return_seconds=False,
            language="ko"
        )

class _AudioStream:
    """ë‚´ë¶€ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ í´ë˜ìŠ¤ (private)"""
    def __init__(self):
        self.queue = queue.Queue()
        self.stream = None

    def init_stream(self):
        if self.stream is None:
            self.stream = sd.InputStream(
                device=AudioConfig.DEVICE,
                blocksize=AudioConfig.CHUNKSIZE,
                channels=AudioConfig.CHANNELS,
                samplerate=AudioConfig.SAMPLERATE, 
                callback=self._audio_callback
            )
            print("ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì´ˆê¸°í™” ì™„ë£Œ")

    def start_stream(self):
        if self.stream is not None:
            self.stream.start()
            print("ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì‹œì‘ë¨")
        else:
            raise RuntimeError("ìŠ¤íŠ¸ë¦¼ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    def stop_stream(self):
        if self.stream is not None:
            self.stream.stop()
            self.stream.close()
            self.stream = None
            print("ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œë¨")

    def process_audio_batch(self, target=AudioConfig.BATCH_SIZE):
        chunks = []
        
        try:
            while len(chunks) < target:
                chunk = self.queue.get(timeout=1.0)
                chunks.append(chunk)
        except queue.Empty:
            pass
            
        return np.concatenate(chunks, axis=0).squeeze() if chunks else None
        
    def _audio_callback(self, indata, frames, time, status):
        if status:
            print(status, file=sys.stderr)
        self.queue.put(indata.copy())


class _AudioActivityDetection:
    """ë‚´ë¶€ ìŒì„± í™œë™ ê°ì§€ í´ë˜ìŠ¤ (private)"""
    def __init__(self, silence_threshold: int = AudioConfig.SILENCE_THRESHOLD):
        self.is_recording = False
        self.speech_buffer = []
        self.speech_id = 0
        self.stop_count = 0
        self.silence_threshold = silence_threshold

    def __call__(self, speech_detected, audio_buffer):
        has_speech = len(speech_detected) > 0
        
        if has_speech:
            if not self.is_recording:
                self.is_recording = True
                self.speech_buffer = []
                print("ğŸ¤ ìŒì„± ì‹œì‘")
            
            self.speech_buffer.append(audio_buffer)
            
            if self.stop_count > 0:
                print(f"ìŒì„± ì¬ê°ì§€ â†’ ë¬´ìŒ ì¹´ìš´íŠ¸ ë¦¬ì…‹ ({self.stop_count} â†’ 0)")
                self.stop_count = 0
            
        else:  # ë¬´ìŒ
            if self.is_recording:
                zero_data = np.zeros_like(audio_buffer)
                self.speech_buffer.append(zero_data)
                self.stop_count += 1
                
                print(f"ì—°ì† ë¬´ìŒ: {self.stop_count}/{self.silence_threshold}")
                
                if self.stop_count >= self.silence_threshold:
                    speech_data = np.concatenate(self.speech_buffer, axis=0)
                    self.is_recording = False
                    self.stop_count = 0
                    self.speech_buffer = []
                    self.speech_id += 1
                    
                    print(f"ğŸ›‘ ì—°ì† {self.silence_threshold}ë²ˆ ë¬´ìŒìœ¼ë¡œ ì¢…ë£Œ")
                    return speech_data

        return None


def _get_vad_model():
    """VAD ëª¨ë¸ ì‹±ê¸€í†¤ getter"""
    global _vad_model
    if _vad_model is None:
        _vad_model = _VADModel()
    return _vad_model


def _listen_and_transcribe():
    """ë‚´ë¶€ í•¨ìˆ˜: ì‹¤ì‹œê°„ ìŒì„± ìˆ˜ì§‘ ë° í…ìŠ¤íŠ¸ ë³€í™˜"""
    vad_model = _get_vad_model()
    stream = _AudioStream()
    stream.init_stream()
    stream.start_stream()
    event_checker = _AudioActivityDetection()

    print("ìŠ¤íŠ¸ë¦¼ ì‹œì‘ë¨ - ë§ì”€í•´ì£¼ì„¸ìš”")

    try:
        while True:
            audio_data = stream.process_audio_batch()
            
            if audio_data is not None:
                speech_timestamps = vad_model.get_speech_timestamps(audio_data)
                result = event_checker(speech_timestamps, audio_data)
                
                if result is not None:
                    print(f"ì €ì¥ëœ ìŒì„± í´ë¦½ {event_checker.speech_id}, ê¸¸ì´: {result.shape}")
                    
                    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                    sf.write("temp_audio.wav", result, samplerate=AudioConfig.SAMPLERATE)

                    # OpenAI Whisper APIë¡œ ì „ì‚¬
                    with open("temp_audio.wav", "rb") as audio_file:
                        response = client.audio.transcriptions.create(
                            model="whisper-1",
                            file=audio_file,
                            language="ko"
                        )

                    transcript_text = response.text
                    print(f"ë³€í™˜ëœ í…ìŠ¤íŠ¸: {transcript_text}")
                    
                    return transcript_text
            else:
                time.sleep(0.1)
                
    finally:
        stream.stop_stream()


# ========== PUBLIC API ==========

def audio2text(mode: str = "stream", wavefile: UploadFile = File(None)) -> str:
    """
    ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í†µí•© API
    
    Args:
        mode: "stream" (ì‹¤ì‹œê°„ ë§ˆì´í¬ ì…ë ¥) ë˜ëŠ” "file" (íŒŒì¼ ì…ë ¥)
        wavefile: modeê°€ "file"ì¼ ë•Œ í•„ìš”í•œ ì˜¤ë””ì˜¤ íŒŒì¼ (UploadFile)
    
    Returns:
        str: ë³€í™˜ëœ í…ìŠ¤íŠ¸
    
    Raises:
        ValueError: modeê°€ "file"ì¸ë° wavefileì´ Noneì¼ ë•Œ
    """
    if mode == "stream":
        return _listen_and_transcribe()
    
    elif mode == "file":
        if wavefile is None:
            raise ValueError("mode='file'ì¼ ë•ŒëŠ” wavefileì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        try:
            # OpenAI Whisper APIë¡œ ì§ì ‘ ì „ì‚¬
            with open(wavefile, "rb") as audio_file:
                response = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="ko"
                )
            
            return response.text
        
        except Exception as e:
            print(f"íŒŒì¼ ì „ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""
    
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” mode: {mode}. 'stream' ë˜ëŠ” 'file'ì„ ì‚¬ìš©í•˜ì„¸ìš”.")


if __name__ == '__main__':
    # CLI ëª¨ë“œ: ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹
    text = audio2text(mode="file", wavefile="temp_audio.wav")
    print(f"\nìµœì¢… ê²°ê³¼: {text}")